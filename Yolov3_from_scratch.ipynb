{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r77XkGTfJ2df"
      },
      "outputs": [],
      "source": [
        "#filters, kernel_size, stride\n",
        "model_config=[\n",
        "    (32,1,1),\n",
        "    (64,3,2),\n",
        "    [\"R\",1],\n",
        "    (128,3,2),\n",
        "    [\"R\",2],\n",
        "    (256,3,2),\n",
        "    [\"R\",8],\n",
        "    (512,3,2),\n",
        "    [\"R\",8],\n",
        "    (1024,3,2),\n",
        "    [\"R\",4], # to this point its a darknet-53\n",
        "    (512,1,1),\n",
        "    (1024,3,1),\n",
        "    (512,1,1),\n",
        "    (1024,3,1),\n",
        "    (512,1,1),\n",
        "    (1024,3,1),\n",
        "    (255,1,1),\n",
        "    \"S\",\n",
        "    (256,1,1),\n",
        "    \"U\",\n",
        "    (256,1,1),\n",
        "    (512,3,1),\n",
        "    (256,1,1),\n",
        "    (512,3,1),\n",
        "    (256,1,1),\n",
        "    (512,3,1),\n",
        "    (255,1,1),\n",
        "    \"S\",\n",
        "    (128,1,1),\n",
        "    \"U\",\n",
        "    (128,1,1),\n",
        "    (256,3,1),\n",
        "    (128,1,1),\n",
        "    (256,3,1),\n",
        "    (128,1,1),\n",
        "    (256,3,1),\n",
        "    (255,1,1),\n",
        "    \"S\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPy4UqcIdTeL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,kernel_size,stride,padding):\n",
        "    super(CNNBlock,self).__init__()\n",
        "    self.cnn = nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
        "    self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.LeakyReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.cnn(x)\n",
        "    x = self.batchnorm(x)\n",
        "    x = self.relu(x)\n",
        "    return x\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self,num_repeats,in_channels):\n",
        "    super(ResidualBlock,self).__init__()\n",
        "    self.num_repeats=num_repeats\n",
        "    self.layers = []\n",
        "    self.create_res_layers(in_channels)\n",
        "\n",
        "  def create_res_layers(self,in_channels):\n",
        "    for _ in range(self.num_repeats):\n",
        "      self.layers.append(\n",
        "          nn.Sequential(\n",
        "                nn.Conv2d(in_channels,in_channels//2,3,1,1),\n",
        "                nn.Conv2d(in_channels//2,in_channels,1,1,0)\n",
        "      ))\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x) + x\n",
        "    return x\n",
        "\n",
        "class ScalingBlock(nn.Module):\n",
        "  def __init__(self,in_channels,num_classes):\n",
        "    super(ScalingBlock,self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.pred  = nn.Sequential(\n",
        "        CNNBlock(in_channels,2*in_channels,kernel_size=3,stride=1,padding=1),\n",
        "        CNNBlock(2*in_channels,3*(self.num_classes + 5),kernel_size=1,stride=1,padding=0)\n",
        "    )\n",
        "\n",
        "  def forward(self,input_x):\n",
        "    x = self.pred(input_x) # x = (N,3*(self.num_classes + 5),width,height)\n",
        "    x = x.reshape(-1,3,self.num_classes + 5,x.shape[2],x.shape[3]) # x = (N,3 , (self.num_classes + 5), width,height)\n",
        "    x = x.permute(0,1,3,4,2) # x = (N, 3 , width,height, (self.num_classes + 5))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_J6i9NLeNhI"
      },
      "outputs": [],
      "source": [
        "class Yolov3(nn.Module):\n",
        "  def __init__(self,model_config,num_classes):\n",
        "    super(Yolov3,self).__init__()\n",
        "    self.num_classes=num_classes\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.create_model(model_config)\n",
        "\n",
        "  def create_model(self,model_config):\n",
        "    in_channels = 3\n",
        "    for layer in model_config:\n",
        "      if isinstance(layer,tuple):\n",
        "        padding = 1 if layer[1]==3 else 0\n",
        "        self.layers.append(CNNBlock(in_channels,layer[0],layer[1],layer[2],padding))\n",
        "        in_channels=layer[0]\n",
        "      elif isinstance(layer,list):\n",
        "        self.layers.append(ResidualBlock(layer[1],in_channels))\n",
        "      elif isinstance(layer,str):\n",
        "        if layer==\"U\":\n",
        "          self.layers.append(nn.Upsample(scale_factor=2))\n",
        "          in_channels=in_channels*3\n",
        "        elif layer==\"S\":\n",
        "          self.layers.append(ScalingBlock(in_channels,self.num_classes))\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    output_pred=[]\n",
        "    route_connection=[]\n",
        "    for layer in self.layers:\n",
        "      if isinstance(layer,ScalingBlock):\n",
        "        output_pred.append(layer(x))\n",
        "        continue\n",
        "\n",
        "      x = layer(x)\n",
        "      if isinstance(layer,ResidualBlock) and layer.num_repeats==8:\n",
        "        route_connection.append(x)\n",
        "      elif isinstance(layer,nn.Upsample):\n",
        "        x = torch.cat([x,route_connection[-1]],dim=1) # dim 1 is for channels\n",
        "        route_connection.pop()\n",
        "    return output_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGpPFMACELWE",
        "outputId": "6c63991a-1e17-4976-9316-42b772ce302d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 13, 13, 25]) torch.Size([2, 3, 26, 26, 25]) torch.Size([2, 3, 52, 52, 25])\n"
          ]
        }
      ],
      "source": [
        "num_classes=20\n",
        "IMG_SIZE=416\n",
        "x=torch.randn(2,3,IMG_SIZE,IMG_SIZE)\n",
        "model=Yolov3(model_config,num_classes=20)\n",
        "pred=model(x)\n",
        "print(pred[0].shape,pred[1].shape,pred[2].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kAo-E7dDp0j"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCuhLlbdyZrX"
      },
      "source": [
        "# Yolo DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z_lJYhcEwyz"
      },
      "outputs": [],
      "source": [
        "from utils import iou_width_height\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image, ImageFile\n",
        "import os\n",
        "import pandas as pd\n",
        "class YoloDataLoader(Dataset):\n",
        "  def __init__(self,train_csv_path,transform,S,img_dir,label_dir,anchors,C=20):\n",
        "    self.transform = transform\n",
        "    self.anchors = [anchors[0] + anchors[1] + anchors[2]]\n",
        "    self.anchor_per_stride = len(self.anchors)//3\n",
        "    self.strides = S\n",
        "    self.ignore_iou_thresh = 0.5\n",
        "    self.label_dir = label_dir\n",
        "    self.annotations = pd.read_csv(train_csv_path)\n",
        "    self.img_dir = img_dir\n",
        "    self.num_anchors = self.anchors.shape[0]\n",
        "    self.num_anchors_per_scale = self.num_anchors // 3\n",
        "    self.C = C\n",
        "\n",
        "  def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n",
        "    bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
        "    img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "    image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "    if self.transform:\n",
        "        augmentations = self.transform(image=image, bboxes=bboxes)\n",
        "        image = augmentations[\"image\"]\n",
        "        bboxes = augmentations[\"bboxes\"]\n",
        "\n",
        "    self.labels = [torch.zeros((self.num_anchors,S,S,6)) for S in self.strides] #  (N,num_anchors,S,S,5+1)\n",
        "\n",
        "    for bbox in bboxes:\n",
        "      box_x,box_y,box_width,box_height,class_label = bbox\n",
        "      anchor_ious = iou_width_height(torch.tensor([box_width,box_height],self.anchors))\n",
        "      anchor_indices = anchor_ious.argsort(descending=True,dim=0)\n",
        "      scale_taken = [False,False,False] # ROW\n",
        "      for iou_index in anchor_indices:\n",
        "        scale_idx = iou_index//self.anchor_per_stride\n",
        "        anchor_on_scale = iou_index%self.anchor_per_stride\n",
        "        S = self.strides[scale_idx]\n",
        "        i,j =  int(box_y*S), int(box_x*S) # row, column\n",
        "        anchor_taken = self.labels[scale_idx][anchor_on_scale,i,j,0]\n",
        "        if not anchor_taken and not scale_taken[scale_idx]:\n",
        "          cell_x,cell_y = box_x*S - j, box_y*S - i\n",
        "          cell_width, cell_height = box_width*S, box_height*S\n",
        "          box_coords = torch.tesnor([cell_x,cell_y,cell_width, cell_height])\n",
        "          scale_taken[scale_idx]=True\n",
        "          self.labels[scale_idx][anchor_on_scale,i,j,0] = 1\n",
        "          self.labels[scale_idx][anchor_on_scale,i,j,1:5]=box_coords\n",
        "          self.labels[scale_idx][anchor_on_scale,i,j,5]=int(class_label)\n",
        "        elif not anchor_taken and anchor_ious[iou_index] > self.ignore_iou_thresh:\n",
        "          self.labels[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore prediction\n",
        "    return self.img, self.labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlWP9CA00BEt",
        "outputId": "8ad1183b-4239-4184-8774-0c2b662f811b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/imgaug/transforms.py:346: FutureWarning: This IAAAffine is deprecated. Please use Affine instead\n",
            "  warnings.warn(\"This IAAAffine is deprecated. Please use Affine instead\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import config\n",
        "def get_loaders(train_csv_path, test_csv_path):\n",
        "\n",
        "    IMAGE_SIZE = config.IMAGE_SIZE\n",
        "    train_dataset = YoloDataLoader(\n",
        "        train_csv_path,\n",
        "        transform=config.train_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=config.IMG_DIR,\n",
        "        label_dir=config.LABEL_DIR,\n",
        "        anchors=config.ANCHORS,\n",
        "    )\n",
        "    test_dataset = YoloDataLoader(\n",
        "        test_csv_path,\n",
        "        transform=config.test_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=config.IMG_DIR,\n",
        "        label_dir=config.LABEL_DIR,\n",
        "        anchors=config.ANCHORS,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    train_eval_dataset = YoloDataLoader(\n",
        "        train_csv_path,\n",
        "        transform=config.test_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=config.IMG_DIR,\n",
        "        label_dir=config.LABEL_DIR,\n",
        "        anchors=config.ANCHORS,\n",
        "    )\n",
        "    train_eval_loader = DataLoader(\n",
        "        dataset=train_eval_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, train_eval_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLK0JGMfyVZO"
      },
      "source": [
        "# Yolo Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4o-1iPJG90Z"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(YoloLoss,self).__init__()\n",
        "    self.mse = nn.MSELoss()\n",
        "    self.bce = nn.BCEWithLogitsLoss()\n",
        "    self.entropy = nn.CrossEntropyLoss()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.lambda_box=10\n",
        "    self.lambda_noobj=10\n",
        "\n",
        "  def forward(self,predictions,target,anchors): # target = (3,S,S,25) # 3 anchors at scale S, (5 + C)\n",
        "\n",
        "    self.anchors = anchors.reshape(1,3,1,1,2)\n",
        "    obj = target[...,0] == 1\n",
        "    noobj = target[...,0] == 0\n",
        "\n",
        "    ## objectiveness loss ##\n",
        "    obj_loss = self.bce(predictions[...,0:1][obj],target[...,0:1][obj])\n",
        "\n",
        "    ## no object loss ##\n",
        "    noobj_loss = self.bce(predictions[...,0:1][noobj],target[...,0:1][noobj])\n",
        "\n",
        "    ## box coordinate loss ##\n",
        "    predictions[...,1:3] = self.sigmoid(predictions[...,1:3])\n",
        "    target[...,3:5] = torch.log(1e-16 + target[...,3:5] / self.anchors)  # for the better gradient flow\n",
        "    box_loss = self.mse(predictions[...,1:5][obj],target[...,1:5][obj])\n",
        "\n",
        "    ## class loss ##\n",
        "    class_loss = self.entropy(predictions[...,5:][obj],target[...,5][obj])\n",
        "\n",
        "    loss = obj_loss + self.lambda_noobj*noobj_loss + self.lambda_box*box_loss + class_loss\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqgsUhOyyRMa"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Oki9mxNPyTpS"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Main file for training Yolo model on Pascal VOC and COCO dataset\n",
        "\"\"\"\n",
        "\n",
        "import config\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from tqdm import tqdm\n",
        "from utils import (\n",
        "    mean_average_precision,\n",
        "    cells_to_bboxes,\n",
        "    get_evaluation_bboxes,\n",
        "    save_checkpoint,\n",
        "    load_checkpoint,\n",
        "    check_class_accuracy,\n",
        "    get_loaders,\n",
        "    plot_couple_examples\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    losses = []\n",
        "    for batch_idx, (x, y) in enumerate(loop):\n",
        "        x = x.to(config.DEVICE)\n",
        "        y0, y1, y2 = (\n",
        "            y[0].to(config.DEVICE),\n",
        "            y[1].to(config.DEVICE),\n",
        "            y[2].to(config.DEVICE),\n",
        "        )\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            out = model(x)\n",
        "            loss = (\n",
        "                loss_fn(out[0], y0, scaled_anchors[0])\n",
        "                + loss_fn(out[1], y1, scaled_anchors[1])\n",
        "                + loss_fn(out[2], y2, scaled_anchors[2])\n",
        "            )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # update progress bar\n",
        "        mean_loss = sum(losses) / len(losses)\n",
        "        loop.set_postfix(loss=mean_loss)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    model = Yolov3(model_config,num_classes=config.NUM_CLASSES).to(config.DEVICE)\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY\n",
        "    )\n",
        "    loss_fn = YoloLoss()\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    train_loader, test_loader, train_eval_loader = get_loaders(\n",
        "        train_csv_path=config.DATASET + \"/train.csv\", test_csv_path=config.DATASET + \"/test.csv\"\n",
        "    )\n",
        "\n",
        "    if config.LOAD_MODEL:\n",
        "        load_checkpoint(\n",
        "            config.CHECKPOINT_FILE, model, optimizer, config.LEARNING_RATE\n",
        "        )\n",
        "\n",
        "    scaled_anchors = (\n",
        "        torch.tensor(config.ANCHORS)\n",
        "        * torch.tensor(config.S).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
        "    ).to(config.DEVICE)\n",
        "\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        #plot_couple_examples(model, test_loader, 0.6, 0.5, scaled_anchors)\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n",
        "\n",
        "        #if config.SAVE_MODEL:\n",
        "        #    save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n",
        "\n",
        "        #print(f\"Currently epoch {epoch}\")\n",
        "        #print(\"On Train Eval loader:\")\n",
        "        #print(\"On Train loader:\")\n",
        "        #check_class_accuracy(model, train_loader, threshold=config.CONF_THRESHOLD)\n",
        "\n",
        "        if epoch > 0 and epoch % 3 == 0:\n",
        "            check_class_accuracy(model, test_loader, threshold=config.CONF_THRESHOLD)\n",
        "            pred_boxes, true_boxes = get_evaluation_bboxes(\n",
        "                test_loader,\n",
        "                model,\n",
        "                iou_threshold=config.NMS_IOU_THRESH,\n",
        "                anchors=config.ANCHORS,\n",
        "                threshold=config.CONF_THRESHOLD,\n",
        "            )\n",
        "            mapval = mean_average_precision(\n",
        "                pred_boxes,\n",
        "                true_boxes,\n",
        "                iou_threshold=config.MAP_IOU_THRESH,\n",
        "                box_format=\"midpoint\",\n",
        "                num_classes=config.NUM_CLASSES,\n",
        "            )\n",
        "            print(f\"MAP: {mapval.item()}\")\n",
        "            model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "TQdpGvF514w_",
        "outputId": "61f166f4-4f75-4b68-e644-65cf15b07615"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-69a72927dac4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     train_loader, test_loader, train_eval_loader = get_loaders(\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mtrain_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_csv_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     )\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mget_loaders\u001b[0;34m(train_csv_path, test_csv_path)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_couple_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_thresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j5wb6IKymYN"
      },
      "source": [
        "# Utils\n",
        "\n",
        "*   Create file utils.py\n",
        "*   Copy paste below content in it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7RuP6MFymww"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def iou_width_height(boxes1, boxes2):\n",
        "\n",
        "    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(\n",
        "        boxes1[..., 1], boxes2[..., 1]\n",
        "    )\n",
        "    union = (\n",
        "        boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n",
        "    )\n",
        "    return intersection / union\n",
        "\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms\n",
        "\n",
        "\n",
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "):\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "\n",
        "def plot_image(image, boxes):\n",
        "    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n",
        "    cmap = plt.get_cmap(\"tab20b\")\n",
        "    class_labels = config.COCO_LABELS if config.DATASET=='COCO' else config.PASCAL_CLASSES\n",
        "    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n",
        "    im = np.array(image)\n",
        "    height, width, _ = im.shape\n",
        "\n",
        "    # Create figure and axes\n",
        "    fig, ax = plt.subplots(1)\n",
        "    # Display the image\n",
        "    ax.imshow(im)\n",
        "    # Create a Rectangle patch\n",
        "    for box in boxes:\n",
        "        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n",
        "        class_pred = box[0]\n",
        "        box = box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width,\n",
        "            box[3] * height,\n",
        "            linewidth=2,\n",
        "            edgecolor=colors[int(class_pred)],\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "        # Add the patch to the Axes\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(\n",
        "            upper_left_x * width,\n",
        "            upper_left_y * height,\n",
        "            s=class_labels[int(class_pred)],\n",
        "            color=\"white\",\n",
        "            verticalalignment=\"top\",\n",
        "            bbox={\"color\": colors[int(class_pred)], \"pad\": 0},\n",
        "        )\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_evaluation_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    anchors,\n",
        "    threshold,\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    # make sure model is in eval before get bboxes\n",
        "    model.eval()\n",
        "    train_idx = 0\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "    for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n",
        "        x = x.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        bboxes = [[] for _ in range(batch_size)]\n",
        "        for i in range(3):\n",
        "            S = predictions[i].shape[2]\n",
        "            anchor = torch.tensor([*anchors[i]]).to(device) * S\n",
        "            boxes_scale_i = cells_to_bboxes(\n",
        "                predictions[i], anchor, S=S, is_preds=True\n",
        "            )\n",
        "            for idx, (box) in enumerate(boxes_scale_i):\n",
        "                bboxes[idx] += box\n",
        "\n",
        "        # we just want one bbox for each label, not one for each scale\n",
        "        true_bboxes = cells_to_bboxes(\n",
        "            labels[2], anchor, S=S, is_preds=False\n",
        "        )\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "def cells_to_bboxes(predictions, anchors, S, is_preds=True):\n",
        "\n",
        "    BATCH_SIZE = predictions.shape[0]\n",
        "    num_anchors = len(anchors)\n",
        "    box_predictions = predictions[..., 1:5]\n",
        "    if is_preds:\n",
        "        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n",
        "        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n",
        "        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n",
        "        scores = torch.sigmoid(predictions[..., 0:1])\n",
        "        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n",
        "    else:\n",
        "        scores = predictions[..., 0:1]\n",
        "        best_class = predictions[..., 5:6]\n",
        "\n",
        "    cell_indices = (\n",
        "        torch.arange(S)\n",
        "        .repeat(predictions.shape[0], 3, S, 1)\n",
        "        .unsqueeze(-1)\n",
        "        .to(predictions.device)\n",
        "    )\n",
        "    x = 1 / S * (box_predictions[..., 0:1] + cell_indices)\n",
        "    y = 1 / S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
        "    w_h = 1 / S * box_predictions[..., 2:4]\n",
        "    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
        "    return converted_bboxes.tolist()\n",
        "\n",
        "def check_class_accuracy(model, loader, threshold):\n",
        "    model.eval()\n",
        "    tot_class_preds, correct_class = 0, 0\n",
        "    tot_noobj, correct_noobj = 0, 0\n",
        "    tot_obj, correct_obj = 0, 0\n",
        "\n",
        "    for idx, (x, y) in enumerate(tqdm(loader)):\n",
        "        x = x.to(config.DEVICE)\n",
        "        with torch.no_grad():\n",
        "            out = model(x)\n",
        "\n",
        "        for i in range(3):\n",
        "            y[i] = y[i].to(config.DEVICE)\n",
        "            obj = y[i][..., 0] == 1 # in paper this is Iobj_i\n",
        "            noobj = y[i][..., 0] == 0  # in paper this is Iobj_i\n",
        "\n",
        "            correct_class += torch.sum(\n",
        "                torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n",
        "            )\n",
        "            tot_class_preds += torch.sum(obj)\n",
        "\n",
        "            obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n",
        "            correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n",
        "            tot_obj += torch.sum(obj)\n",
        "            correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n",
        "            tot_noobj += torch.sum(noobj)\n",
        "\n",
        "    print(f\"Class accuracy is: {(correct_class/(tot_class_preds+1e-16))*100:2f}%\")\n",
        "    print(f\"No obj accuracy is: {(correct_noobj/(tot_noobj+1e-16))*100:2f}%\")\n",
        "    print(f\"Obj accuracy is: {(correct_obj/(tot_obj+1e-16))*100:2f}%\")\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def get_mean_std(loader):\n",
        "    # var[X] = E[X**2] - E[X]**2\n",
        "    channels_sum, channels_sqrd_sum, num_batches = 0, 0, 0\n",
        "\n",
        "    for data, _ in tqdm(loader):\n",
        "        channels_sum += torch.mean(data, dim=[0, 2, 3])\n",
        "        channels_sqrd_sum += torch.mean(data ** 2, dim=[0, 2, 3])\n",
        "        num_batches += 1\n",
        "\n",
        "    mean = channels_sum / num_batches\n",
        "    std = (channels_sqrd_sum / num_batches - mean ** 2) ** 0.5\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=config.DEVICE)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    # If we don't do this then it will just have learning rate of old checkpoint\n",
        "    # and it will lead to many hours of debugging \\:\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "\n",
        "def get_loaders(train_csv_path, test_csv_path):\n",
        "    # from dataset import YOLODataset\n",
        "\n",
        "    IMAGE_SIZE = config.IMAGE_SIZE\n",
        "    train_dataset = YOLODataset(\n",
        "        train_csv_path,\n",
        "        transform=config.train_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=config.IMG_DIR,\n",
        "        label_dir=config.LABEL_DIR,\n",
        "        anchors=config.ANCHORS,\n",
        "    )\n",
        "    test_dataset = YOLODataset(\n",
        "        test_csv_path,\n",
        "        transform=config.test_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=config.IMG_DIR,\n",
        "        label_dir=config.LABEL_DIR,\n",
        "        anchors=config.ANCHORS,\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    train_eval_dataset = YOLODataset(\n",
        "        train_csv_path,\n",
        "        transform=config.test_transforms,\n",
        "        S=[IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8],\n",
        "        img_dir=config.IMG_DIR,\n",
        "        label_dir=config.LABEL_DIR,\n",
        "        anchors=config.ANCHORS,\n",
        "    )\n",
        "    train_eval_loader = DataLoader(\n",
        "        dataset=train_eval_dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=config.PIN_MEMORY,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, train_eval_loader\n",
        "\n",
        "def plot_couple_examples(model, loader, thresh, iou_thresh, anchors):\n",
        "    model.eval()\n",
        "    x, y = next(iter(loader))\n",
        "    x = x.to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        out = model(x)\n",
        "        bboxes = [[] for _ in range(x.shape[0])]\n",
        "        for i in range(3):\n",
        "            batch_size, A, S, _, _ = out[i].shape\n",
        "            anchor = anchors[i]\n",
        "            boxes_scale_i = cells_to_bboxes(\n",
        "                out[i], anchor, S=S, is_preds=True\n",
        "            )\n",
        "            for idx, (box) in enumerate(boxes_scale_i):\n",
        "                bboxes[idx] += box\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        nms_boxes = non_max_suppression(\n",
        "            bboxes[i], iou_threshold=iou_thresh, threshold=thresh, box_format=\"midpoint\",\n",
        "        )\n",
        "        plot_image(x[i].permute(1,2,0).detach().cpu(), nms_boxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrMbhLyayCSh"
      },
      "source": [
        "# Config file\n",
        "\n",
        "\n",
        "\n",
        "*   Create file config.py\n",
        "*   Copy paste below content in it\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P6Ywt33U1Sa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# #Config.py\n",
        "# import albumentations as A\n",
        "# import cv2\n",
        "# import torch\n",
        "\n",
        "# from albumentations.pytorch import ToTensorV2\n",
        "# # from utils import seed_everything\n",
        "\n",
        "# DATASET = 'PASCAL_VOC'\n",
        "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# # seed_everything()  # If you want deterministic behavior\n",
        "# NUM_WORKERS = 4\n",
        "# BATCH_SIZE = 32\n",
        "# IMAGE_SIZE = 416\n",
        "# NUM_CLASSES = 20\n",
        "# LEARNING_RATE = 1e-5\n",
        "# WEIGHT_DECAY = 1e-4\n",
        "# NUM_EPOCHS = 100\n",
        "# CONF_THRESHOLD = 0.05\n",
        "# MAP_IOU_THRESH = 0.5\n",
        "# NMS_IOU_THRESH = 0.45\n",
        "# S = [IMAGE_SIZE // 32, IMAGE_SIZE // 16, IMAGE_SIZE // 8]\n",
        "# PIN_MEMORY = True\n",
        "# LOAD_MODEL = True\n",
        "# SAVE_MODEL = True\n",
        "# CHECKPOINT_FILE = \"checkpoint.pth.tar\"\n",
        "# IMG_DIR = DATASET + \"/images/\"\n",
        "# LABEL_DIR = DATASET + \"/labels/\"\n",
        "\n",
        "# ANCHORS = [\n",
        "#     [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
        "#     [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
        "#     [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
        "# ]  # Note these have been rescaled to be between [0, 1]\n",
        "\n",
        "\n",
        "# scale = 1.1\n",
        "# train_transforms = A.Compose(\n",
        "#     [\n",
        "#         A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n",
        "#         A.PadIfNeeded(\n",
        "#             min_height=int(IMAGE_SIZE * scale),\n",
        "#             min_width=int(IMAGE_SIZE * scale),\n",
        "#             border_mode=cv2.BORDER_CONSTANT,\n",
        "#         ),\n",
        "#         A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n",
        "#         A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n",
        "#         A.OneOf(\n",
        "#             [\n",
        "#                 A.ShiftScaleRotate(\n",
        "#                     rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
        "#                 ),\n",
        "#                 A.IAAAffine(shear=15, p=0.5, mode=\"constant\"),\n",
        "#             ],\n",
        "#             p=1.0,\n",
        "#         ),\n",
        "#         A.HorizontalFlip(p=0.5),\n",
        "#         A.Blur(p=0.1),\n",
        "#         A.CLAHE(p=0.1),\n",
        "#         A.Posterize(p=0.1),\n",
        "#         A.ToGray(p=0.1),\n",
        "#         A.ChannelShuffle(p=0.05),\n",
        "#         A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "#         ToTensorV2(),\n",
        "#     ],\n",
        "#     bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n",
        "# )\n",
        "# test_transforms = A.Compose(\n",
        "#     [\n",
        "#         A.LongestMaxSize(max_size=IMAGE_SIZE),\n",
        "#         A.PadIfNeeded(\n",
        "#             min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n",
        "#         ),\n",
        "#         A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
        "#         ToTensorV2(),\n",
        "#     ],\n",
        "#     bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n",
        "# )\n",
        "\n",
        "# PASCAL_CLASSES = [\n",
        "#     \"aeroplane\",\n",
        "#     \"bicycle\",\n",
        "#     \"bird\",\n",
        "#     \"boat\",\n",
        "#     \"bottle\",\n",
        "#     \"bus\",\n",
        "#     \"car\",\n",
        "#     \"cat\",\n",
        "#     \"chair\",\n",
        "#     \"cow\",\n",
        "#     \"diningtable\",\n",
        "#     \"dog\",\n",
        "#     \"horse\",\n",
        "#     \"motorbike\",\n",
        "#     \"person\",\n",
        "#     \"pottedplant\",\n",
        "#     \"sheep\",\n",
        "#     \"sofa\",\n",
        "#     \"train\",\n",
        "#     \"tvmonitor\"\n",
        "# ]\n",
        "\n",
        "# COCO_LABELS = ['person',\n",
        "#  'bicycle',\n",
        "#  'car',\n",
        "#  'motorcycle',\n",
        "#  'airplane',\n",
        "#  'bus',\n",
        "#  'train',\n",
        "#  'truck',\n",
        "#  'boat',\n",
        "#  'traffic light',\n",
        "#  'fire hydrant',\n",
        "#  'stop sign',\n",
        "#  'parking meter',\n",
        "#  'bench',\n",
        "#  'bird',\n",
        "#  'cat',\n",
        "#  'dog',\n",
        "#  'horse',\n",
        "#  'sheep',\n",
        "#  'cow',\n",
        "#  'elephant',\n",
        "#  'bear',\n",
        "#  'zebra',\n",
        "#  'giraffe',\n",
        "#  'backpack',\n",
        "#  'umbrella',\n",
        "#  'handbag',\n",
        "#  'tie',\n",
        "#  'suitcase',\n",
        "#  'frisbee',\n",
        "#  'skis',\n",
        "#  'snowboard',\n",
        "#  'sports ball',\n",
        "#  'kite',\n",
        "#  'baseball bat',\n",
        "#  'baseball glove',\n",
        "#  'skateboard',\n",
        "#  'surfboard',\n",
        "#  'tennis racket',\n",
        "#  'bottle',\n",
        "#  'wine glass',\n",
        "#  'cup',\n",
        "#  'fork',\n",
        "#  'knife',\n",
        "#  'spoon',\n",
        "#  'bowl',\n",
        "#  'banana',\n",
        "#  'apple',\n",
        "#  'sandwich',\n",
        "#  'orange',\n",
        "#  'broccoli',\n",
        "#  'carrot',\n",
        "#  'hot dog',\n",
        "#  'pizza',\n",
        "#  'donut',\n",
        "#  'cake',\n",
        "#  'chair',\n",
        "#  'couch',\n",
        "#  'potted plant',\n",
        "#  'bed',\n",
        "#  'dining table',\n",
        "#  'toilet',\n",
        "#  'tv',\n",
        "#  'laptop',\n",
        "#  'mouse',\n",
        "#  'remote',\n",
        "#  'keyboard',\n",
        "#  'cell phone',\n",
        "#  'microwave',\n",
        "#  'oven',\n",
        "#  'toaster',\n",
        "#  'sink',\n",
        "#  'refrigerator',\n",
        "#  'book',\n",
        "#  'clock',\n",
        "#  'vase',\n",
        "#  'scissors',\n",
        "#  'teddy bear',\n",
        "#  'hair drier',\n",
        "#  'toothbrush'\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG06nqlWKCXh"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}